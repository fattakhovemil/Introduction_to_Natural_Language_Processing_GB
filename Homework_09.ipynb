{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkBiEAIl7SuW"
      },
      "source": [
        "# Языковое моделирование"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1gxya76dhqW",
        "outputId": "83a1dafb-e2dc-42cc-c367-0d09bbe8bd29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o_estefhPI0",
        "outputId": "066c83e9-b4d4-4706-de8f-1910cec21d3f"
      },
      "source": [
        "!pip install pymorphy2\n",
        "!pip install stop-words"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 33.7 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=97adcb1b03cbe37c66ff7b8c5d12f1e0332c57f76c60562fba8f4366362712b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-PZ-63CdiCE"
      },
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from stop_words import get_stop_words\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blMV3YGahag7"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/evgenyi_onegin.txt'"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHfhHp3Rd5oN"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3w9PTo4hkee",
        "outputId": "15f736e3-aa22-430a-83bf-1225d47fc4e4"
      },
      "source": [
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 286984 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM7N4-othpAp",
        "outputId": "bf92d096-4ae5-47e6-919a-155bb4df7150"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Александр Сергеевич Пушкин\n",
            "\n",
            "                                Евгений Онегин\n",
            "                                Роман в стихах\n",
            "\n",
            "                        Не мысля гордый свет забавить,\n",
            "                        Вниманье дружбы возлюбя,\n",
            "                        Хотел бы я тебе представить\n",
            "                        Залог достойнее тебя,\n",
            "                        Достойнее души прекрасной,\n",
            "                        Святой исполненной мечты,\n",
            "                        Поэзии живой и ясной,\n",
            "                        Высо\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYKyS3_hkwL"
      },
      "source": [
        "text = text.split('\\n\\n')"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "fHmpW56XhkzJ",
        "outputId": "f0a4fb52-73b9-4dfc-9e4a-2a41cd4f857b"
      },
      "source": [
        "text[14]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'                        Мы все учились понемногу\\n                        Чему-нибудь и как-нибудь,\\n                        Так воспитаньем, слава богу,\\n                        У нас немудрено блеснуть.\\n                        Онегин был по мненью многих\\n                        (Судей решительных и строгих)\\n                        Ученый малый, но педант:\\n                        Имел он счастливый талант\\n                        Без принужденья в разговоре\\n                        Коснуться до всего слегка,\\n                        С ученым видом знатока\\n                        Хранить молчанье в важном споре\\n                        И возбуждать улыбку дам\\n                        Огнем нежданных эпиграмм.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIt5gZEKhk1x",
        "outputId": "24ecc414-1b84-491a-e4b7-dc9cbdb20afd"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "782"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_f7TQCFhk4U",
        "outputId": "0787c42b-dbf9-4295-bd26-1e44c9106a65"
      },
      "source": [
        "text_only = []\n",
        "for strofa in text:\n",
        "    if len(strofa) < 350:\n",
        "        continue\n",
        "    else:\n",
        "        text_only.append(strofa)\n",
        "len(text_only)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "376"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-zvLacExhk6x",
        "outputId": "c874b4b6-b9e9-42f6-a4c0-683bef097466"
      },
      "source": [
        "data = pd.DataFrame(text_only)\n",
        "data = data.rename(columns={0: \"text\"})\n",
        "data"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Не мысля гордый свет з...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Мой дядя самых честны...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Так думал молодой пове...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Служив отлично благоро...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Когда же юности мятежн...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>А счастье было так воз...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>Она ушла. Стоит Евгени...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>Кто б ни был ты, о мой...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Прости ж и ты, мой спу...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>Но те, которым в дружн...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>376 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text\n",
              "0                            Не мысля гордый свет з...\n",
              "1                            \"Мой дядя самых честны...\n",
              "2                            Так думал молодой пове...\n",
              "3                            Служив отлично благоро...\n",
              "4                            Когда же юности мятежн...\n",
              "..                                                 ...\n",
              "371                          А счастье было так воз...\n",
              "372                          Она ушла. Стоит Евгени...\n",
              "373                          Кто б ни был ты, о мой...\n",
              "374                          Прости ж и ты, мой спу...\n",
              "375                          Но те, которым в дружн...\n",
              "\n",
              "[376 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC_mlvC_hk9Y"
      },
      "source": [
        "sw = set(get_stop_words(\"ru\"))\n",
        "exclude = set(punctuation)\n",
        "morpher = MorphAnalyzer()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcoh1ux4hk_1"
      },
      "source": [
        "def exclude_punctuation(txt):\n",
        "    txt = \"\".join(c for c in txt if c not in exclude)    \n",
        "    txt = re.sub(\"\\n\", \" \\n\", txt)    \n",
        "    return txt"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgzriILphlCM"
      },
      "source": [
        "def preprocess_text(txt, morph = False):\n",
        "    txt = str(txt)\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(\"\\n\", \"zzz\", txt)\n",
        "    new_txt =[]\n",
        "    for word in txt.split():\n",
        "        if word == \"zzz\":\n",
        "            word = \" \\n\"\n",
        "            \n",
        "        else:\n",
        "            if morph:\n",
        "                word = morpher.parse(word)[0].normal_form\n",
        "            else:\n",
        "                pass\n",
        "        new_txt.append(word)\n",
        "\n",
        "    return new_txt"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "AzcKVSvbjy2l",
        "outputId": "c3d7ec2c-11c1-4cd8-ade9-bcbd2c80c52d"
      },
      "source": [
        "data['text_splited'] = data['text'].apply(exclude_punctuation)\n",
        "data['text_splited'] = data['text_splited'].apply(preprocess_text, morph = False)\n",
        "\n",
        "data"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_splited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Не мысля гордый свет з...</td>\n",
              "      <td>[не, мысля, гордый, свет, забавить,  \\n, внима...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Мой дядя самых честны...</td>\n",
              "      <td>[мой, дядя, самых, честных, правил,  \\n, когда...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Так думал молодой пове...</td>\n",
              "      <td>[так, думал, молодой, повеса,  \\n, летя, в, пы...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Служив отлично благоро...</td>\n",
              "      <td>[служив, отлично, благородно,  \\n, долгами, жи...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Когда же юности мятежн...</td>\n",
              "      <td>[когда, же, юности, мятежной,  \\n, пришла, евг...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>А счастье было так воз...</td>\n",
              "      <td>[а, счастье, было, так, возможно,  \\n, так, бл...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>Она ушла. Стоит Евгени...</td>\n",
              "      <td>[она, ушла, стоит, евгений,  \\n, как, будто, г...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>Кто б ни был ты, о мой...</td>\n",
              "      <td>[кто, б, ни, был, ты, о, мой, читатель,  \\n, д...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Прости ж и ты, мой спу...</td>\n",
              "      <td>[прости, ж, и, ты, мой, спутник, странный,  \\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>Но те, которым в дружн...</td>\n",
              "      <td>[но, те, которым, в, дружной, встрече,  \\n, я,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>376 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text                                       text_splited\n",
              "0                            Не мысля гордый свет з...  [не, мысля, гордый, свет, забавить,  \\n, внима...\n",
              "1                            \"Мой дядя самых честны...  [мой, дядя, самых, честных, правил,  \\n, когда...\n",
              "2                            Так думал молодой пове...  [так, думал, молодой, повеса,  \\n, летя, в, пы...\n",
              "3                            Служив отлично благоро...  [служив, отлично, благородно,  \\n, долгами, жи...\n",
              "4                            Когда же юности мятежн...  [когда, же, юности, мятежной,  \\n, пришла, евг...\n",
              "..                                                 ...                                                ...\n",
              "371                          А счастье было так воз...  [а, счастье, было, так, возможно,  \\n, так, бл...\n",
              "372                          Она ушла. Стоит Евгени...  [она, ушла, стоит, евгений,  \\n, как, будто, г...\n",
              "373                          Кто б ни был ты, о мой...  [кто, б, ни, был, ты, о, мой, читатель,  \\n, д...\n",
              "374                          Прости ж и ты, мой спу...  [прости, ж, и, ты, мой, спутник, странный,  \\n...\n",
              "375                          Но те, которым в дружн...  [но, те, которым, в, дружной, встрече,  \\n, я,...\n",
              "\n",
              "[376 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRRy97u3j4zb"
      },
      "source": [
        "def get_w2i_i2w(column_data):\n",
        "    \n",
        "    dump = list(column_data.values)\n",
        "    dump_txt_split = []\n",
        "    for sublist in dump:\n",
        "        for item in sublist:\n",
        "            dump_txt_split.append(item)\n",
        "\n",
        "\n",
        "    vocab = sorted(set(dump_txt_split))           \n",
        "            \n",
        "    # Creating a mapping from unique characters to indices\n",
        "    word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "    idx2word = np.array(vocab)\n",
        "    \n",
        "    print(len(vocab), len(word2idx), len(idx2word))\n",
        "    return word2idx, idx2word"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VW067zAj49p",
        "outputId": "0b9f0ae6-1c6f-41de-e3ba-180a9141122e"
      },
      "source": [
        "word2idx, idx2word = get_w2i_i2w(data['text_splited'])        \n",
        "data['int_text_splited'] = data['text_splited'].apply(lambda x: [word2idx[c] for c in x])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8427 8427 8427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "4l07PHXFj5Af",
        "outputId": "be3c0882-3635-4836-facb-86d74907ce5b"
      },
      "source": [
        "data"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_splited</th>\n",
              "      <th>int_text_splited</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Не мысля гордый свет з...</td>\n",
              "      <td>[не, мысля, гордый, свет, забавить,  \\n, внима...</td>\n",
              "      <td>[3817, 3634, 1358, 6327, 2071, 0, 878, 1844, 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"Мой дядя самых честны...</td>\n",
              "      <td>[мой, дядя, самых, честных, правил,  \\n, когда...</td>\n",
              "      <td>[3487, 1912, 6292, 8129, 5394, 0, 2788, 3817, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Так думал молодой пове...</td>\n",
              "      <td>[так, думал, молодой, повеса,  \\n, летя, в, пы...</td>\n",
              "      <td>[7249, 1867, 3505, 4930, 0, 3102, 565, 5872, 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Служив отлично благоро...</td>\n",
              "      <td>[служив, отлично, благородно,  \\n, долгами, жи...</td>\n",
              "      <td>[6660, 4535, 325, 0, 1720, 2041, 1919, 4512, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Когда же юности мятежн...</td>\n",
              "      <td>[когда, же, юности, мятежной,  \\n, пришла, евг...</td>\n",
              "      <td>[2788, 1980, 8363, 3638, 0, 5651, 1915, 5222, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>А счастье было так воз...</td>\n",
              "      <td>[а, счастье, было, так, возможно,  \\n, так, бл...</td>\n",
              "      <td>[98, 7202, 550, 7249, 924, 0, 7249, 380, 4072,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>Она ушла. Стоит Евгени...</td>\n",
              "      <td>[она, ушла, стоит, евгений,  \\n, как, будто, г...</td>\n",
              "      <td>[4371, 7870, 7006, 1914, 0, 2637, 515, 1462, 5...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>Кто б ни был ты, о мой...</td>\n",
              "      <td>[кто, б, ни, был, ты, о, мой, читатель,  \\n, д...</td>\n",
              "      <td>[2960, 160, 4037, 546, 7548, 4141, 3487, 8153,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>Прости ж и ты, мой спу...</td>\n",
              "      <td>[прости, ж, и, ты, мой, спутник, странный,  \\n...</td>\n",
              "      <td>[5751, 1951, 2446, 7548, 3487, 6916, 7049, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>Но те, которым в дружн...</td>\n",
              "      <td>[но, те, которым, в, дружной, встрече,  \\n, я,...</td>\n",
              "      <td>[4072, 7309, 2876, 565, 1849, 1132, 0, 8375, 7...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>376 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  ...                                   int_text_splited\n",
              "0                            Не мысля гордый свет з...  ...  [3817, 3634, 1358, 6327, 2071, 0, 878, 1844, 9...\n",
              "1                            \"Мой дядя самых честны...  ...  [3487, 1912, 6292, 8129, 5394, 0, 2788, 3817, ...\n",
              "2                            Так думал молодой пове...  ...  [7249, 1867, 3505, 4930, 0, 3102, 565, 5872, 3...\n",
              "3                            Служив отлично благоро...  ...  [6660, 4535, 325, 0, 1720, 2041, 1919, 4512, 0...\n",
              "4                            Когда же юности мятежн...  ...  [2788, 1980, 8363, 3638, 0, 5651, 1915, 5222, ...\n",
              "..                                                 ...  ...                                                ...\n",
              "371                          А счастье было так воз...  ...  [98, 7202, 550, 7249, 924, 0, 7249, 380, 4072,...\n",
              "372                          Она ушла. Стоит Евгени...  ...  [4371, 7870, 7006, 1914, 0, 2637, 515, 1462, 5...\n",
              "373                          Кто б ни был ты, о мой...  ...  [2960, 160, 4037, 546, 7548, 4141, 3487, 8153,...\n",
              "374                          Прости ж и ты, мой спу...  ...  [5751, 1951, 2446, 7548, 3487, 6916, 7049, 0, ...\n",
              "375                          Но те, которым в дружн...  ...  [4072, 7309, 2876, 565, 1849, 1132, 0, 8375, 7...\n",
              "\n",
              "[376 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiQSgip_j5DD"
      },
      "source": [
        "def get_all_int(column_data):\n",
        "    \n",
        "    int_dump = column_data.values\n",
        "    all_txt_as_int = []\n",
        "\n",
        "    for sublist in int_dump:\n",
        "        for item in sublist:\n",
        "            all_txt_as_int.append(item)\n",
        "    all_txt_as_int = np.array(all_txt_as_int)    \n",
        "    \n",
        "    return all_txt_as_int\n",
        "\n",
        "all_txt_as_int = get_all_int(data['int_text_splited'])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C2bAt8yj5F1",
        "outputId": "9d7cf0db-3056-4319-dd1c-6c096dbfb095"
      },
      "source": [
        "# The maximum length sentence you want for a single input in characters\n",
        "seq_length = 50\n",
        "examples_per_epoch = len(all_txt_as_int)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "word_dataset = tf.data.Dataset.from_tensor_slices(all_txt_as_int)\n",
        "\n",
        "for i in word_dataset.take(15):\n",
        "    print(idx2word[i.numpy()])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "не\n",
            "мысля\n",
            "гордый\n",
            "свет\n",
            "забавить\n",
            " \n",
            "\n",
            "вниманье\n",
            "дружбы\n",
            "возлюбя\n",
            " \n",
            "\n",
            "хотел\n",
            "бы\n",
            "я\n",
            "тебе\n",
            "представить\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIt1BSvVkv-2"
      },
      "source": [
        "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSjYf0YZkmF8"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT-u-GcZkmIV",
        "outputId": "dbf8d580-2431-458a-9959-c3493c91793c"
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "    print('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
        "    print('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'не мысля гордый свет забавить  \\n вниманье дружбы возлюбя  \\n хотел бы я тебе представить  \\n залог достойнее тебя  \\n достойнее души прекрасной  \\n святой исполненной мечты  \\n поэзии живой и ясной  \\n высоких дум и простоты  \\n но так и быть рукой пристрастной  \\n прими собранье пестрых глав  \\n'\n",
            "Target data: 'мысля гордый свет забавить  \\n вниманье дружбы возлюбя  \\n хотел бы я тебе представить  \\n залог достойнее тебя  \\n достойнее души прекрасной  \\n святой исполненной мечты  \\n поэзии живой и ясной  \\n высоких дум и простоты  \\n но так и быть рукой пристрастной  \\n прими собранье пестрых глав  \\n полусмешных'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRoRr4L-kmLB",
        "outputId": "68a466d0-8541-4196-d10a-7396b4866ec7"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 50), (64, 50)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8rV55ZlkmNt"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(idx2word)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEMqhL-tkmPv"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  \n",
        "    inputs = tf.keras.layers.Input(batch_input_shape=[batch_size, None])\n",
        "\n",
        "    x =     tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "    print(x.shape)\n",
        "    x1 = tf.keras.layers.GRU(rnn_units,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform')(x)\n",
        "    x = tf.keras.layers.concatenate([x,x1], axis=-1)\n",
        "    \n",
        "    print(x.shape)\n",
        "    x2 = tf.keras.layers.GRU(rnn_units+embedding_dim,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform')(x)\n",
        "    x = tf.keras.layers.add([x,x2])\n",
        "    \n",
        "    print(x.shape)\n",
        "    x3 = tf.keras.layers.GRU(rnn_units+embedding_dim,\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform')(x)   \n",
        "    \n",
        "    x = tf.keras.layers.add([x,x3])   \n",
        "    x = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    print(x.shape)\n",
        "\n",
        "    model =tf.keras. Model(inputs=inputs, outputs=x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuHL80XgkmSH",
        "outputId": "17a16747-b2c8-465c-f7fa-50b6a77e137c"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, None, 128)\n",
            "(64, None, 1152)\n",
            "(64, None, 1152)\n",
            "(64, None, 8427)\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKaxFyE9kmUV",
        "outputId": "b759936c-40c0-488c-9c17-6949b549d42f"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 50, 8427) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDwzztHKlHZi",
        "outputId": "ea40c38f-1a0d-4711-afe0-bc51aea1a7a8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(64, None)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (64, None, 128)      1078656     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     (64, None, 1024)     3545088     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (64, None, 1152)     0           embedding_2[0][0]                \n",
            "                                                                 gru_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_7 (GRU)                     (64, None, 1152)     7969536     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (64, None, 1152)     0           concatenate_2[0][0]              \n",
            "                                                                 gru_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "gru_8 (GRU)                     (64, None, 1152)     7969536     add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (64, None, 1152)     0           add_4[0][0]                      \n",
            "                                                                 gru_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (64, None, 8427)     9716331     add_5[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 30,279,147\n",
            "Trainable params: 30,279,147\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CmNhHgDlHcP",
        "outputId": "7ecd5ff0-c9b1-4e87-8c9e-b3b5ec67e192"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 50, 8427)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       9.039446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmIyCsFflHe7"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAxbJaZVlVfn",
        "outputId": "dee04554-a270-4921-d7e1-34a00242b0c9"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    period=20,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpFjT9EslVim"
      },
      "source": [
        "EPOCHS = 200"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp9ZI0-blVlD",
        "outputId": "677f36bd-b58b-4438-92e2-68a08c450142"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "8/8 [==============================] - 4s 174ms/step - loss: 8.7691\n",
            "Epoch 2/200\n",
            "8/8 [==============================] - 1s 172ms/step - loss: 7.7917\n",
            "Epoch 3/200\n",
            "8/8 [==============================] - 1s 172ms/step - loss: 7.0464\n",
            "Epoch 4/200\n",
            "8/8 [==============================] - 1s 172ms/step - loss: 6.9264\n",
            "Epoch 5/200\n",
            "8/8 [==============================] - 1s 172ms/step - loss: 6.8775\n",
            "Epoch 6/200\n",
            "8/8 [==============================] - 1s 174ms/step - loss: 6.8392\n",
            "Epoch 7/200\n",
            "8/8 [==============================] - 1s 175ms/step - loss: 6.7691\n",
            "Epoch 8/200\n",
            "8/8 [==============================] - 1s 173ms/step - loss: 6.6002\n",
            "Epoch 9/200\n",
            "8/8 [==============================] - 1s 175ms/step - loss: 6.4225\n",
            "Epoch 10/200\n",
            "8/8 [==============================] - 1s 177ms/step - loss: 6.2515\n",
            "Epoch 11/200\n",
            "8/8 [==============================] - 1s 175ms/step - loss: 6.0796\n",
            "Epoch 12/200\n",
            "8/8 [==============================] - 1s 175ms/step - loss: 5.8800\n",
            "Epoch 13/200\n",
            "8/8 [==============================] - 1s 176ms/step - loss: 5.6608\n",
            "Epoch 14/200\n",
            "8/8 [==============================] - 1s 177ms/step - loss: 5.3928\n",
            "Epoch 15/200\n",
            "8/8 [==============================] - 1s 178ms/step - loss: 5.1111\n",
            "Epoch 16/200\n",
            "8/8 [==============================] - 1s 178ms/step - loss: 4.8302\n",
            "Epoch 17/200\n",
            "8/8 [==============================] - 1s 178ms/step - loss: 4.4862\n",
            "Epoch 18/200\n",
            "8/8 [==============================] - 1s 179ms/step - loss: 4.1235\n",
            "Epoch 19/200\n",
            "8/8 [==============================] - 1s 179ms/step - loss: 3.7494\n",
            "Epoch 20/200\n",
            "8/8 [==============================] - 1s 179ms/step - loss: 3.3622\n",
            "Epoch 21/200\n",
            "8/8 [==============================] - 1s 180ms/step - loss: 3.0112\n",
            "Epoch 22/200\n",
            "8/8 [==============================] - 1s 178ms/step - loss: 2.6798\n",
            "Epoch 23/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 2.3972\n",
            "Epoch 24/200\n",
            "8/8 [==============================] - 1s 181ms/step - loss: 2.1024\n",
            "Epoch 25/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 1.8674\n",
            "Epoch 26/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 1.6529\n",
            "Epoch 27/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 1.4826\n",
            "Epoch 28/200\n",
            "8/8 [==============================] - 2s 183ms/step - loss: 1.3109\n",
            "Epoch 29/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 1.1671\n",
            "Epoch 30/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 1.0584\n",
            "Epoch 31/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 0.9566\n",
            "Epoch 32/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 0.8571\n",
            "Epoch 33/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.7641\n",
            "Epoch 34/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.7005\n",
            "Epoch 35/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.6292\n",
            "Epoch 36/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 0.5725\n",
            "Epoch 37/200\n",
            "8/8 [==============================] - 1s 189ms/step - loss: 0.5304\n",
            "Epoch 38/200\n",
            "8/8 [==============================] - 2s 184ms/step - loss: 0.4984\n",
            "Epoch 39/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.4674\n",
            "Epoch 40/200\n",
            "8/8 [==============================] - 2s 184ms/step - loss: 0.4305\n",
            "Epoch 41/200\n",
            "8/8 [==============================] - 2s 186ms/step - loss: 0.3980\n",
            "Epoch 42/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.3745\n",
            "Epoch 43/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.3637\n",
            "Epoch 44/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 0.3372\n",
            "Epoch 45/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.3248\n",
            "Epoch 46/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.3226\n",
            "Epoch 47/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.3038\n",
            "Epoch 48/200\n",
            "8/8 [==============================] - 1s 183ms/step - loss: 0.2934\n",
            "Epoch 49/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.2876\n",
            "Epoch 50/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.2814\n",
            "Epoch 51/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.2698\n",
            "Epoch 52/200\n",
            "8/8 [==============================] - 2s 186ms/step - loss: 0.2625\n",
            "Epoch 53/200\n",
            "8/8 [==============================] - 2s 186ms/step - loss: 0.2575\n",
            "Epoch 54/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.2524\n",
            "Epoch 55/200\n",
            "8/8 [==============================] - 2s 184ms/step - loss: 0.2378\n",
            "Epoch 56/200\n",
            "8/8 [==============================] - 1s 182ms/step - loss: 0.2327\n",
            "Epoch 57/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.9338\n",
            "Epoch 58/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.6268\n",
            "Epoch 59/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.4819\n",
            "Epoch 60/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.3487\n",
            "Epoch 61/200\n",
            "8/8 [==============================] - 2s 186ms/step - loss: 0.2857\n",
            "Epoch 62/200\n",
            "8/8 [==============================] - 2s 186ms/step - loss: 0.2494\n",
            "Epoch 63/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.2201\n",
            "Epoch 64/200\n",
            "8/8 [==============================] - 2s 185ms/step - loss: 0.2112\n",
            "Epoch 65/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1981\n",
            "Epoch 66/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1901\n",
            "Epoch 67/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1877\n",
            "Epoch 68/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.1792\n",
            "Epoch 69/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.1749\n",
            "Epoch 70/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.1750\n",
            "Epoch 71/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.1700\n",
            "Epoch 72/200\n",
            "8/8 [==============================] - 2s 187ms/step - loss: 0.1607\n",
            "Epoch 73/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1564\n",
            "Epoch 74/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.1558\n",
            "Epoch 75/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.1583\n",
            "Epoch 76/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.1509\n",
            "Epoch 77/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.1482\n",
            "Epoch 78/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.1475\n",
            "Epoch 79/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.1451\n",
            "Epoch 80/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1425\n",
            "Epoch 81/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1393\n",
            "Epoch 82/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.1422\n",
            "Epoch 83/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1353\n",
            "Epoch 84/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.1326\n",
            "Epoch 85/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.1336\n",
            "Epoch 86/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.1284\n",
            "Epoch 87/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1268\n",
            "Epoch 88/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1274\n",
            "Epoch 89/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1244\n",
            "Epoch 90/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1269\n",
            "Epoch 91/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1263\n",
            "Epoch 92/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1160\n",
            "Epoch 93/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1235\n",
            "Epoch 94/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.1129\n",
            "Epoch 95/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.1123\n",
            "Epoch 96/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1073\n",
            "Epoch 97/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.1080\n",
            "Epoch 98/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1075\n",
            "Epoch 99/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1072\n",
            "Epoch 100/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1093\n",
            "Epoch 101/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.1071\n",
            "Epoch 102/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1061\n",
            "Epoch 103/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.1026\n",
            "Epoch 104/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0977\n",
            "Epoch 105/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0982\n",
            "Epoch 106/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0977\n",
            "Epoch 107/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0955\n",
            "Epoch 108/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0943\n",
            "Epoch 109/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0960\n",
            "Epoch 110/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0923\n",
            "Epoch 111/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0880\n",
            "Epoch 112/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0889\n",
            "Epoch 113/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0878\n",
            "Epoch 114/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0860\n",
            "Epoch 115/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0867\n",
            "Epoch 116/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0840\n",
            "Epoch 117/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0860\n",
            "Epoch 118/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0825\n",
            "Epoch 119/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0859\n",
            "Epoch 120/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0833\n",
            "Epoch 121/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0808\n",
            "Epoch 122/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0811\n",
            "Epoch 123/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0805\n",
            "Epoch 124/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0800\n",
            "Epoch 125/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0786\n",
            "Epoch 126/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0790\n",
            "Epoch 127/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0759\n",
            "Epoch 128/200\n",
            "8/8 [==============================] - 2s 195ms/step - loss: 0.0756\n",
            "Epoch 129/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0733\n",
            "Epoch 130/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0712\n",
            "Epoch 131/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0748\n",
            "Epoch 132/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0729\n",
            "Epoch 133/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0716\n",
            "Epoch 134/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0723\n",
            "Epoch 135/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0719\n",
            "Epoch 136/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0679\n",
            "Epoch 137/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0696\n",
            "Epoch 138/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0692\n",
            "Epoch 139/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0666\n",
            "Epoch 140/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0674\n",
            "Epoch 141/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0653\n",
            "Epoch 142/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0659\n",
            "Epoch 143/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0626\n",
            "Epoch 144/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0632\n",
            "Epoch 145/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0640\n",
            "Epoch 146/200\n",
            "8/8 [==============================] - 2s 196ms/step - loss: 0.0662\n",
            "Epoch 147/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0637\n",
            "Epoch 148/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0599\n",
            "Epoch 149/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0616\n",
            "Epoch 150/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0634\n",
            "Epoch 151/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0605\n",
            "Epoch 152/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0610\n",
            "Epoch 153/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0573\n",
            "Epoch 154/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0588\n",
            "Epoch 155/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0575\n",
            "Epoch 156/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0590\n",
            "Epoch 157/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0563\n",
            "Epoch 158/200\n",
            "8/8 [==============================] - 2s 196ms/step - loss: 0.0567\n",
            "Epoch 159/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0579\n",
            "Epoch 160/200\n",
            "8/8 [==============================] - 2s 196ms/step - loss: 0.0574\n",
            "Epoch 161/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0578\n",
            "Epoch 162/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0540\n",
            "Epoch 163/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0570\n",
            "Epoch 164/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0550\n",
            "Epoch 165/200\n",
            "8/8 [==============================] - 2s 195ms/step - loss: 0.0550\n",
            "Epoch 166/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0554\n",
            "Epoch 167/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0547\n",
            "Epoch 168/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0559\n",
            "Epoch 169/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0565\n",
            "Epoch 170/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0548\n",
            "Epoch 171/200\n",
            "8/8 [==============================] - 2s 195ms/step - loss: 0.0515\n",
            "Epoch 172/200\n",
            "8/8 [==============================] - 2s 195ms/step - loss: 0.0525\n",
            "Epoch 173/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0509\n",
            "Epoch 174/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0545\n",
            "Epoch 175/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0526\n",
            "Epoch 176/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0525\n",
            "Epoch 177/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0503\n",
            "Epoch 178/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0516\n",
            "Epoch 179/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0533\n",
            "Epoch 180/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0511\n",
            "Epoch 181/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0529\n",
            "Epoch 182/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0498\n",
            "Epoch 183/200\n",
            "8/8 [==============================] - 2s 188ms/step - loss: 0.0480\n",
            "Epoch 184/200\n",
            "8/8 [==============================] - 2s 189ms/step - loss: 0.0503\n",
            "Epoch 185/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0509\n",
            "Epoch 186/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0499\n",
            "Epoch 187/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0488\n",
            "Epoch 188/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0496\n",
            "Epoch 189/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0479\n",
            "Epoch 190/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0494\n",
            "Epoch 191/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0486\n",
            "Epoch 192/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0488\n",
            "Epoch 193/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0506\n",
            "Epoch 194/200\n",
            "8/8 [==============================] - 2s 192ms/step - loss: 0.0475\n",
            "Epoch 195/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0470\n",
            "Epoch 196/200\n",
            "8/8 [==============================] - 2s 193ms/step - loss: 0.0478\n",
            "Epoch 197/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0471\n",
            "Epoch 198/200\n",
            "8/8 [==============================] - 2s 194ms/step - loss: 0.0478\n",
            "Epoch 199/200\n",
            "8/8 [==============================] - 2s 191ms/step - loss: 0.0473\n",
            "Epoch 200/200\n",
            "8/8 [==============================] - 2s 190ms/step - loss: 0.0471\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Fp-thd_-lVnh",
        "outputId": "99822108-1768-4db8-e14b-ea69733004a0"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_200'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ZxJ0xllVpk",
        "outputId": "835e17f6-2afd-4222-bb4e-50d689af6408"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, None, 128)\n",
            "(1, None, 1152)\n",
            "(1, None, 1152)\n",
            "(1, None, 8427)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A68li6J3lHh4"
      },
      "source": [
        "def generate_text(model, start_string, tmprt):\n",
        "    # Evaluation step (generating text using the learned model)\n",
        "    \n",
        "    start_string = exclude_punctuation(start_string)\n",
        "    #print(start_string)\n",
        "    start_string_asis = preprocess_text(start_string, morph = False)\n",
        "    \n",
        "    # Number of characters to generate\n",
        "    num_generate = 30\n",
        "\n",
        "    # Converting our start string to numbers (vectorizing)\n",
        "    input_eval = [word2idx[s] for s in start_string_asis]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Empty string to store our results\n",
        "    text_generated = []\n",
        "\n",
        "    # Low temperature results in more predictable text.\n",
        "    # Higher temperature results in more surprising text.\n",
        "    # Experiment to find the best setting.\n",
        "    temperature = tmprt\n",
        "    # Here batch size == 1\n",
        "    model.reset_states()\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # using a categorical distribution to predict the character returned by the model\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Pass the predicted character as the next input to the model\n",
        "        # along with the previous hidden state\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "    return (start_string + ' '.join(text_generated))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYvKB5P5lHkd",
        "outputId": "4a3d13a5-6c3a-4edb-e3cf-5ba8b83e096f"
      },
      "source": [
        "print(generate_text(model, start_string=u\"счастье \", tmprt=1.3))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "счастье пожалуйте мне бедный милой  \n",
            " скрыв никто без двор  \n",
            " одно вид его на цельным окнам тени ходят  \n",
            " мелькают профили пустяков  \n",
            " с ним клубился смиренной  \n",
            " невинной прелести\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-lbxOUKkmW4"
      },
      "source": [
        "start_string= 'А счастье было так '"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHJLLb_qpC4w",
        "outputId": "27d8bb99-55d4-4c43-eb36-fca2c4ae34c4"
      },
      "source": [
        "print(generate_text(model, start_string=start_string, tmprt=1.3))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "А счастье было так душеньки путь  \n",
            " нетерпеливому герою  \n",
            " отселе в думу погружен  \n",
            " глядел на грозный пламень он прощай свидетель падшей славы  \n",
            " петровский замок ну не стой  \n",
            " пошел уже столпы\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEt2FcV3oZZx",
        "outputId": "7d7ee610-cacb-4c4d-d966-3fbb87b79200"
      },
      "source": [
        "print(generate_text(model, start_string=start_string, tmprt=1))"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "А счастье было так ступить  \n",
            " приподнялася грудь ланиты  \n",
            " мгновенным пламенем покрыты  \n",
            " дыханье замерло в устах  \n",
            " и в слухе шум и блеск в очах  \n",
            " настанет ночь луна обходит  \n",
            " дозором\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xZuT6ozofDW",
        "outputId": "2bd4ad09-4ab7-4655-8a7f-108cd632c07b"
      },
      "source": [
        "print(generate_text(model, start_string=start_string, tmprt=0.001))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "А счастье было так ступить  \n",
            " приподнялася грудь ланиты  \n",
            " мгновенным пламенем покрыты  \n",
            " дыханье замерло в устах  \n",
            " и в слухе шум и блеск в очах  \n",
            " настанет ночь луна обходит  \n",
            " дозором\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0MmtxIHoj0J",
        "outputId": "706df75f-b828-4498-e793-877b19cfba6c"
      },
      "source": [
        "print(generate_text(model, start_string=start_string, tmprt=5))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "А счастье было так рооr пополам обретут альбом вслед ужели стремит татьяна бестолкова живит жизни моих лет полет  \n",
            " такою поясок дам хваленых небесных прелестный господа деле воскресла родном увы тани охлаждающая потопленные унижусь\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}