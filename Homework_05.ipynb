{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRL86Xl29r9W"
   },
   "source": [
    "# Part-of-Speech разметка, NER, извлечение отношений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xNMViGM9wc3"
   },
   "source": [
    "## Задание 1. Написать теггер на данных с русским языком\n",
    "\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjrfjVzk97DW",
    "outputId": "5955826d-d2e2-4b32-a6e1-2af323b50c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyconll\n",
      "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RxVPSR9a97GA"
   },
   "outputs": [],
   "source": [
    "import pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p3V0kTd897Ie",
    "outputId": "1c164926-a634-45ff-94fd-8b636f4e07cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-25 04:49:52--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81039282 (77M) [text/plain]\n",
      "Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  77.28M   176MB/s    in 0.4s    \n",
      "\n",
      "2021-07-25 04:49:53 (176 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81039282/81039282]\n",
      "\n",
      "--2021-07-25 04:49:53--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10902738 (10M) [text/plain]\n",
      "Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n",
      "\n",
      "./datasets/ru_synta 100%[===================>]  10.40M  38.2MB/s    in 0.3s    \n",
      "\n",
      "2021-07-25 04:49:54 (38.2 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10902738/10902738]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir datasets\n",
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VeTx4V4197LY",
    "outputId": "f772c6ea-e573-4b1e-d563-bac7532a38fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/names.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "from nltk.corpus import names\n",
    "import nltk\n",
    "nltk.download('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Co5pGBIcC37m"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tUFRqelQONR5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xxTBkmfe-zgj"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SKrvSNB-zjQ",
    "outputId": "a42f31a0-cb36-4bb5-c948-a30479e180eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npHB-tXf_VJk"
   },
   "source": [
    "### 1.1 проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey6z-oUI-zqw",
    "outputId": "4a8fffb2-4f7e-4912-e8c2-964b1a2e2e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])\n",
    "    \n",
    "    \n",
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSETqemP-zvg",
    "outputId": "7a6bf5dc-28b4-407b-c889-b659fe98e731"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23568564014423887"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NOUN')\n",
    "default_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7Ke_BhNA_-Q",
    "outputId": "02ad52a3-65be-4dc5-80ec-4e836271354a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB8wa_-lBABS",
    "outputId": "b05f8757-3468-4fcb-b31c-1b248d8a18d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBASDm92BADc",
    "outputId": "f48f4ec3-4dde-441d-e010-3b3c43ad45ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vKQ2JcuBAF8",
    "outputId": "f3ea3d0d-24ca-41c8-fb80-c59c289ed48c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9119991237825633"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NOUN') \n",
    "tag = backoff_tagger(fdata_train,  \n",
    "                     [\n",
    "                      UnigramTagger, \n",
    "                      BigramTagger, \n",
    "                      TrigramTagger\n",
    "                     ],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "tag.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwThLuSXCmWp"
   },
   "source": [
    "## 1.2 написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KhlaNsDWBAIO",
    "outputId": "10bd1a8c-a1ec-4c5b-9f77-a6f20e491912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "        \n",
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label) \n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAiXtSRYBAK0",
    "outputId": "929c2d7d-21de-4512-effc-8fa3bd2c96ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.943644053516665\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9471826239342163\n",
      "(871526, 149809)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9487749806221144\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU01r_C5BAMt",
    "outputId": "727ab69e-bc5f-440f-f0ba-2929646f7816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.7531847133757962\n",
      "(871526, 1048576)\n",
      "<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.7722255922892866\n",
      "(871526, 99485)\n",
      "<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.7532605398847437\n"
     ]
    }
   ],
   "source": [
    "for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n",
    "\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "    \n",
    "\n",
    "    X_train = coder.fit_transform(train_tok)\n",
    "    X_test = coder.transform(test_tok)\n",
    "    \n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)    \n",
    "    \n",
    "    \n",
    "    print(X_train.shape)\n",
    "    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    print(vectorizer, accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SpEa4TaCuhf",
    "outputId": "02a728da-0929-40ef-82ac-edb2c2cb3868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871526, 1198385)\n",
      "TfidfVectorizer_char + HashingVectorizer_word : 0.9441327132409935\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(with_mean=False)\n",
    "coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n",
    "\n",
    "X_train_1 = coder_1.fit_transform(train_tok)\n",
    "X_test_1 = coder_1.transform(test_tok)\n",
    "\n",
    "X_train_2 = coder_2.fit_transform(train_tok)\n",
    "X_test_2 = coder_2.transform(test_tok)\n",
    "\n",
    "\n",
    "X_train = hstack((X_train_1,X_train_2))\n",
    "X_test = hstack((X_test_1,X_test_2))\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)    \n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuUHDflfNggP"
   },
   "source": [
    "__Выводы:__\n",
    "\n",
    "Для nltk.tag лучший вариант это: Комбинация из DefaultTagger UnigramTagger BigramTagger TrigramTagger\n",
    "0.9119991237825633\n",
    "\n",
    "Для Vectorizer лучший вариант это: Комбинация из LogisticRegression поверх TfidfVectorizer при условии analyzer='char'\n",
    "0.9487749806221144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxThe_cF9wR3"
   },
   "source": [
    "## Задание 2. Проверить насколько хорошо работает NER\n",
    "данные брать из http://www.labinform.ru/pub/named_entities/\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы:\n",
    " - передаём в сетку токен и его соседей\n",
    " - передаём в сетку только токен\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1q-UfUCT1um"
   },
   "source": [
    "## 2.1 взять нер из nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wMSLGJad9rG7",
    "outputId": "e072af48-9038-4a0e-e960-b22ef61b65b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting corus\n",
      "  Downloading corus-0.9.0-py3-none-any.whl (83 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |████                            | 10 kB 26.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 20 kB 23.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 30 kB 12.0 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 40 kB 9.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 51 kB 4.8 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 61 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 71 kB 5.9 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 81 kB 6.0 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 83 kB 1.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: corus\n",
      "Successfully installed corus-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TtjCVUsmOWWX"
   },
   "outputs": [],
   "source": [
    "import corus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "myTuCMoeO6Em"
   },
   "outputs": [],
   "source": [
    "from corus import load_ne5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FipubRb7OWZJ",
    "outputId": "65df0ff4-2c27-48c7-ec94-b2f7be78cff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-25 05:29:43--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
      "Resolving www.labinform.ru (www.labinform.ru)... 80.240.100.4\n",
      "Connecting to www.labinform.ru (www.labinform.ru)|80.240.100.4|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1899530 (1.8M) [application/zip]\n",
      "Saving to: ‘collection5.zip’\n",
      "\n",
      "collection5.zip     100%[===================>]   1.81M   443KB/s    in 4.2s    \n",
      "\n",
      "2021-07-25 05:29:47 (443 KB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w0Ino_5YQJF1",
    "outputId": "eeb6467f-586c-4e4c-b77f-417f4f2a5dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcUTAyvKS2Me",
    "outputId": "2fd4e787-f01e-4641-a491-f80032167a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collection5.zip  datasets  sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "kSbtz49EQJIy"
   },
   "outputs": [],
   "source": [
    "path = 'Collection5/'\n",
    "records = load_ne5(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "5L3X0IzISKgn"
   },
   "outputs": [],
   "source": [
    "document = next(records).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "jgRhwYwjSlsL",
    "outputId": "b52d9ff4-e158-4a08-b90d-acc97e07b445"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Главный борец с мафией в СНГ задержан за хищение 46 миллионов долларов\\r\\n\\r\\nПо подозрению в хищении 46 миллионов долларов в Москве задержан директор Бюро по координации борьбы с оргпреступностью и другими опасными преступлениями в государствах СНГ (БКБОП) генерал-лейтенант милиции Александр Боков. По словам представителя СК РФ Владимира Маркина, уголовное дело возбуждено по статье \"мошенничество\", сообщает РИА Новости.\\r\\n\\r\\nДо работы в исполнительном комитете СНГ, Боков руководил организационно-инспекторским управлением МВД РФ. По словам Маркина, генерал был задержан в ночь на 19 января.\\r\\n\\r\\nРанее источник в правоохранительных органах рассказал агентству \"Интерфакс\", что Бокова задержали за вымогательство и хищение 10 миллионов долларов у руководителя одной из крупных транспортных компаний.\\r\\n\\r\\nЕще один источник заявил РАПСИ, что о преступной деятельности милицейского генерала стало известно при разработке оперативных материалов. Вместе с Боковым были задержаны трое его сообщников.\\r\\n\\r\\nМежду тем в сообщении СК РФ говорится о задержании трех человек, включая Бокова. Сообщниками генерала названы директор Международного фонда развития казачества Михаил Креймер и директор ООО \"Стройбетон\" Сергей Степанов.\\r\\n\\r\\nЕще один источник \"Интерфакса\" утверждает, что генерал был также причастен к рейдерским захватам. По словам собеседника агентства, Боков с сообщниками вымогали 35 миллионов долларов у некоего банкира, угрожая ему и его семье физической расправой. Кроме того, по сведениям источника, на полученные преступным путем деньги задержанные приобретали недвижимость в Подмосковье, на Кипре и в Великобритании.\\r\\n\\r\\nВ то же время, по данным СК РФ, генерал и его сообщники требовали 46 миллионов долларов у некоего предпринимателя, которому они обещали оказать содействие в покупке контрольного пакета акций транспортной компании. В 2006 году бизнесмен заплатил 4,5 миллиона долларов за оказание этой \"услуги\", но не получил обещанной поддержки.\\r\\n\\r\\nПо сведениям \"Интерфакса\", Бокова и его сообщников изобличили российские спецслужбы. Источники говорят, что в деле о вымогательстве могут появиться и другие фигуранты, так как генерал МВД вовлек в преступную деятельность многих работников правоохранительных органов и контролирующих их структур.\\r\\n\\r\\nБКБОП действует при совете глав МВД стран-участниц СНГ. Оно было создано в 1993 году. Штаб-квартира бюро расположена в Москве. '"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbMfR0rASlut",
    "outputId": "0a70c7fe-5cb4-4798-e624-c1b50df47d1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('БКБОП', 'ORGANIZATION'),\n",
       " ('Боков', 'PERSON'),\n",
       " ('Бокова', 'PERSON'),\n",
       " ('Боковым', 'PERSON'),\n",
       " ('Главный', 'PERSON'),\n",
       " ('Источники', 'PERSON'),\n",
       " ('Кроме', 'PERSON'),\n",
       " ('Михаил Креймер', 'PERSON'),\n",
       " ('Москве', 'PERSON'),\n",
       " ('СНГ', 'ORGANIZATION'),\n",
       " ('Сергей Степанов', 'PERSON')}"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJt3uMd7T47m"
   },
   "source": [
    "## 2.2. проверить deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "log7mU1VSlxg"
   },
   "outputs": [],
   "source": [
    "# !pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не пошла установка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYu_1z9OUZF4"
   },
   "source": [
    "## 2.3 написать свой нер попробовать разные подходы:\n",
    "- передаём в сетку токен и его соседей\n",
    "- передаём в сетку только токен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FR2sPTFOVX2h",
    "outputId": "0848fd0c-15eb-465a-903c-769bb7eb2589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting razdel\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: razdel\n",
      "Successfully installed razdel-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MMhyzUlVSl2Y"
   },
   "outputs": [],
   "source": [
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "mAJjv18i6QWk"
   },
   "outputs": [],
   "source": [
    "records = corus.load_ne5(path)\n",
    "words_docs = []\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "       \n",
    "        result = 'None'        \n",
    "        \n",
    "        for item in rec.spans:            \n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'PER'):\n",
    "                result = 'PER'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'ORG'):\n",
    "                result = 'ORG'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'MEDIA'):\n",
    "                result = 'MEDIA'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'LOC'):\n",
    "                result = 'LOC'\n",
    "                break\n",
    "            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'GEOPOLIT'):\n",
    "                result = 'GEOPOLIT'\n",
    "                break\n",
    "                \n",
    "    \n",
    "        words.append([token.text, result])\n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1k8RAMja6QZo"
   },
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyYYKXfp6QcR",
    "outputId": "58e6441d-ab6c-48b2-c020-3405ceeb622f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None        219214\n",
       "PER          21200\n",
       "ORG          13651\n",
       "LOC           4568\n",
       "GEOPOLIT      4356\n",
       "MEDIA         2482\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2IIOlMJW6Qey",
    "outputId": "1b1f8585-a2f8-423c-997f-d86a9ae76365"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Главный</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>борец</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>с</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>мафией</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>в</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word   tag\n",
       "0  Главный  None\n",
       "1    борец  None\n",
       "2        с  None\n",
       "3   мафией  None\n",
       "4        в  None"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NBnCTkAL6QhS"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xdVw5yXg6QjJ"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ZJjCF01R6QlO"
   },
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "GcJOq4vxWc6r"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "2xt16YZCWc9H"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "        return input_data\n",
    "\n",
    "def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n",
    "    \n",
    "    vocab_size = 30000\n",
    "    #seq_len = 1\n",
    "\n",
    "    vectorize_layer = TextVectorization(\n",
    "        standardize=custom_standardization,\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=seq_len)\n",
    "\n",
    "\n",
    "    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "    text_data = train_data.map(lambda x, y: x)\n",
    "    vectorize_layer.adapt(text_data)\n",
    "    return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "htTHBq75Vpyy"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "\n",
    "class modelNER(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(modelNER, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim)\n",
    "        self.gPool = GlobalMaxPooling1D()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.fc2 = Dense(50, activation='relu')\n",
    "        self.fc3 = Dense(len(df_words['tag'].value_counts()), activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        pool_x = self.gPool(x)\n",
    "        \n",
    "        fc_x = self.fc1(pool_x)\n",
    "        fc_x = self.fc2(fc_x)\n",
    "        \n",
    "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
    "        return self.fc3(concat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N3nZoESDVp1X",
    "outputId": "53a7da33-b24f-4a9a-a100-3fb2de36f6e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 272s 22ms/step - loss: 0.2840 - accuracy: 0.9172 - val_loss: 0.2037 - val_accuracy: 0.9402\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 315s 25ms/step - loss: 0.1223 - accuracy: 0.9633 - val_loss: 0.2131 - val_accuracy: 0.9421\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 309s 25ms/step - loss: 0.1077 - accuracy: 0.9657 - val_loss: 0.2350 - val_accuracy: 0.9416\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 308s 25ms/step - loss: 0.1029 - accuracy: 0.9665 - val_loss: 0.2633 - val_accuracy: 0.9413\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 306s 25ms/step - loss: 0.1004 - accuracy: 0.9670 - val_loss: 0.2722 - val_accuracy: 0.9345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd533048650>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVG4rNAZVp4G",
    "outputId": "d16efab9-9c8b-47df-807d-bd4d5ed4437e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12444/12444 [==============================] - 307s 25ms/step - loss: 0.2939 - accuracy: 0.9142 - val_loss: 0.2122 - val_accuracy: 0.9394\n",
      "Epoch 2/5\n",
      "12444/12444 [==============================] - 295s 24ms/step - loss: 0.1240 - accuracy: 0.9631 - val_loss: 0.2587 - val_accuracy: 0.8865\n",
      "Epoch 3/5\n",
      "12444/12444 [==============================] - 296s 24ms/step - loss: 0.1090 - accuracy: 0.9656 - val_loss: 0.2676 - val_accuracy: 0.8870\n",
      "Epoch 4/5\n",
      "12444/12444 [==============================] - 292s 23ms/step - loss: 0.1040 - accuracy: 0.9664 - val_loss: 0.2411 - val_accuracy: 0.9410\n",
      "Epoch 5/5\n",
      "12444/12444 [==============================] - 278s 22ms/step - loss: 0.1011 - accuracy: 0.9668 - val_loss: 0.2390 - val_accuracy: 0.9411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd5353db150>"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 30000\n",
    "vectorize_layer = data_prep(train_data, seq_len = 3, vocab_size = vocab_size)\n",
    "\n",
    "\n",
    "mmodel = modelNER()\n",
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "mmodel.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG6xCuD_a8TP"
   },
   "source": [
    "#### Вывод.\n",
    "\n",
    "Длина последовательности практически не влияет на результат"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
